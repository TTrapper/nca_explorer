<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neuromusical Cellular Automata</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel="stylesheet">
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            background: #1a1a2e;
            color: #eee;
            font-family: 'Segoe UI', system-ui, sans-serif;
            min-height: 100vh;
            padding: 20px;
        }
        h1 {
            font-family: 'Press Start 2P', cursive;
            font-size: 1.8em;
            color: #4fc3f7;
            margin-bottom: 15px;
            text-align: center;
            letter-spacing: 2px;
            line-height: 1.5;
            text-shadow: 0 0 20px rgba(79, 195, 247, 0.4);
        }
        .subtitle { color: #888; text-align: center; margin-bottom: 20px; }

        .main-container {
            max-width: 1200px;
            margin: 0 auto;
        }

        .viewers {
            display: flex;
            justify-content: center;
            gap: 40px;
            margin-bottom: 20px;
            flex-wrap: wrap;
        }

        .viewer {
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .viewer-label {
            font-size: 1.1em;
            margin-bottom: 10px;
            color: #4fc3f7;
        }

        canvas {
            border: 2px solid #4fc3f7;
            background: #000;
            image-rendering: pixelated;
        }

        .context-section {
            background: #16213e;
            padding: 15px;
            border-radius: 10px;
            margin-bottom: 20px;
        }

        .context-section h3 {
            color: #4fc3f7;
            margin-bottom: 10px;
            font-size: 0.9em;
        }

        .context-frames {
            display: flex;
            gap: 10px;
            justify-content: center;
        }

        .context-frame {
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .context-frame span {
            font-size: 0.75em;
            color: #888;
            margin-top: 5px;
        }

        .controls {
            background: #16213e;
            padding: 20px;
            border-radius: 10px;
            display: flex;
            flex-direction: column;
            gap: 15px;
        }

        .control-row {
            display: flex;
            gap: 15px;
            align-items: center;
            justify-content: center;
            flex-wrap: wrap;
        }

        button {
            background: #4fc3f7;
            color: #1a1a2e;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            font-weight: bold;
            font-size: 0.9em;
        }

        button:hover { background: #81d4fa; }
        button:disabled { background: #555; cursor: not-allowed; }
        button.active-mode { background: #fff; }

        .slider-group {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .slider-group label {
            color: #aaa;
            font-size: 0.9em;
            min-width: 80px;
        }

        .slider-group input[type="range"] {
            width: 150px;
        }

        .slider-group .value {
            color: #4fc3f7;
            font-family: monospace;
            min-width: 40px;
        }

        .info {
            display: flex;
            gap: 30px;
            justify-content: center;
            font-size: 0.9em;
            color: #aaa;
        }

        .info span {
            color: #4fc3f7;
        }

        .status {
            text-align: center;
            font-size: 0.85em;
            color: #888;
            margin-top: 10px;
        }

        #connStatus.connected { color: #4caf50; }
        #connStatus.disconnected { color: #f44336; }

        .piano-keyboard { position: relative; height: 120px; display: flex; justify-content: center; }
        .piano-key {
            position: relative; border: 1px solid #333; border-radius: 0 0 4px 4px;
            cursor: pointer; display: flex; flex-direction: column;
            justify-content: flex-end; align-items: center; padding-bottom: 4px;
            font-size: 0.65em; font-weight: bold; user-select: none;
        }
        .piano-key.white { width: 44px; height: 110px; background: #556; color: #999; z-index: 1; }
        .piano-key.black { width: 30px; height: 70px; background: #111; color: #666; z-index: 2; margin-left: -15px; margin-right: -15px; }
        .piano-key.filled { color: #4fc3f7; }
        .piano-key.filled.white { background: #668; }
        .piano-key.filled.black { background: #224; }
        .piano-key.selected.white { background: #4fc3f7; color: #1a1a2e; box-shadow: 0 0 10px #4fc3f7; }
        .piano-key.selected.black { background: #4fc3f7; color: #1a1a2e; box-shadow: 0 0 10px #4fc3f7; }
        .piano-key .key-hint { font-size: 1.1em; }

        .instrument-select {
            background: #1a1a2e; color: #4fc3f7; border: 1px solid #4fc3f7;
            border-radius: 5px; padding: 6px 10px; font-size: 0.85em;
            cursor: pointer; outline: none;
        }
        .instrument-select:hover { background: #16213e; }
        .instrument-loading { color: #888; font-size: 0.8em; }


        .noise-meter {
            display: flex; flex-direction: column; align-items: center;
            gap: 4px; margin-left: 10px; user-select: none;
        }
        .noise-meter-label { font-size: 0.7em; color: #888; text-transform: uppercase; letter-spacing: 0.05em; }
        .noise-meter-track {
            width: 18px; height: 90px; background: #222; border-radius: 4px;
            border: 1px solid #444; position: relative; overflow: hidden;
        }
        .noise-meter-fill {
            position: absolute; bottom: 0; left: 0; right: 0;
            background: linear-gradient(to top, #4fc3f7, #f44336);
            border-radius: 0 0 3px 3px; transition: height 0.1s;
        }
        .noise-meter-value { font-size: 0.75em; color: #4fc3f7; font-family: monospace; }

        .article {
            max-width: 720px;
            margin: 60px auto 40px;
            line-height: 1.7;
            font-size: 1.05em;
        }
        .article h2 {
            color: #4fc3f7;
            margin-top: 48px;
            margin-bottom: 12px;
            font-size: 1.6em;
            border-bottom: 1px solid #333;
            padding-bottom: 8px;
        }
        .article h3 {
            color: #81d4fa;
            margin-top: 32px;
            margin-bottom: 10px;
            font-size: 1.2em;
        }
        .article p {
            color: #ccc;
            margin-bottom: 16px;
        }
        .article hr {
            border: none;
            border-top: 1px solid #333;
            margin: 48px 0;
        }
        .article ul {
            color: #ccc;
            margin-bottom: 16px;
            padding-left: 24px;
        }
        .article li {
            margin-bottom: 8px;
        }
        .article-img {
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 16px;
            margin: 24px 0;
        }
        .article-img img {
            max-width: 100%;
            image-rendering: pixelated;
            border: 1px solid #444;
            border-radius: 4px;
        }
        .gallery-table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
        }
        .gallery-table td {
            text-align: center;
            padding: 8px;
            vertical-align: middle;
        }
        .gallery-table img {
            image-rendering: pixelated;
            border: 1px solid #444;
            border-radius: 4px;
        }
        .article code {
            background: #2a2a4e;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: monospace;
        }
        .article strong {
            color: #4fc3f7;
        }
        .article em {
            color: #aaa;
            font-style: italic;
        }

        /* Real-time audio section */
        .rt-section {
            margin-top: 60px;
            padding: 30px;
            background: linear-gradient(135deg, #16213e 0%, #1a1a2e 100%);
            border-radius: 15px;
            border: 1px solid #4fc3f7;
        }
        .rt-section h2 {
            color: #4fc3f7;
            margin-bottom: 10px;
            font-size: 1.8em;
        }
        .rt-section .rt-subtitle {
            color: #888;
            margin-bottom: 25px;
        }
        .rt-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
        }
        .rt-displays {
            display: flex;
            gap: 30px;
            justify-content: center;
            flex-wrap: wrap;
        }
        .rt-display {
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        .rt-display canvas {
            border: 2px solid #4fc3f7;
            border-radius: 8px;
            background: #000;
            image-rendering: pixelated;
        }
        .rt-display .label {
            margin-top: 8px;
            color: #4fc3f7;
            font-size: 0.9em;
        }
        .rt-controls {
            display: flex;
            gap: 15px;
            align-items: center;
            flex-wrap: wrap;
            justify-content: center;
        }
        .rt-controls button {
            background: #4fc3f7;
            color: #1a1a2e;
            border: none;
            padding: 12px 24px;
            border-radius: 8px;
            cursor: pointer;
            font-weight: bold;
            font-size: 1em;
            transition: all 0.2s;
        }
        .rt-controls button:hover { background: #81d4fa; }
        .rt-controls button:disabled { background: #555; cursor: not-allowed; }
        .rt-controls button.active { background: #f44336; }
        #rtMicStatus {
            color: #4caf50;
            font-size: 0.9em;
            min-width: 80px;
        }
        #rtMicStatus.error { color: #f44336; }
    </style>
</head>
<body>
    <div class="main-container">
        <h1>Neuromusical Cellular Automata</h1>
        <p class="subtitle">Play the piano to explore the space of Neural Cellular Automata. Each note samples a latent space which then generates an NCA on the fly!</p>

        <div class="viewers">
            <div class="viewer" id="specViewer" style="display: block;">
                <div class="viewer-label">Spectrogram</div>
                <img id="specImg" width="256" height="256" style="border: 2px solid #4fc3f7; background: #000; image-rendering: pixelated;" />
            </div>
            <div class="viewer">
                <div class="viewer-label">NCA Prediction</div>
                <canvas id="ncaCanvas" width="256" height="256"></canvas>
            </div>
        </div>

        <div class="controls-container" style="display: flex; justify-content: center;">
            <div class="controls">

                <div class="control-row">
                    <button id="playPauseBtn">Pause</button>
                    <button id="stepBtn">Step</button>
                    <button id="resetBtn">Reset</button>
                    
                </div>

                <div class="control-row">
                    <button id="randomLatentBtn">Random Latent</button>
                </div>


                <div class="control-row" style="margin-top: 10px;">
                    <div style="display: flex; flex-direction: column; align-items: center; margin-right: 15px;">
                        <select id="instrumentSelect" class="instrument-select"></select>
                        <span id="instrumentStatus" class="instrument-loading"></span>
                    </div>
                    <div class="piano-keyboard" id="pianoKeyboard"></div>
                    <div class="noise-meter">
                        <div class="noise-meter-label">Perturbation</div>
                        <div class="noise-meter-track">
                            <div class="noise-meter-fill" id="noiseMeterFill"></div>
                        </div>
                        <div class="noise-meter-value" id="noiseMeterValue">1.0</div>
                    </div>
                </div>

                <div class="info">
                    
                    <div>Frame: <span id="frameNum">0</span></div>
                    <div>Latent: <span id="latentSource">encoded</span></div>
                </div>
            </div>
        </div>

        <div class="status">
            <span id="connStatus" class="disconnected">Loading...</span>
        </div>

        <div class="article">
            <hr>
            
            <p>The demo above <em>generates</em> a unique Neural Cellular Automaton for each sound you play. The NCA runs in real-time, creating evolving patterns based on the audio's spectral fingerprint, and facilitates the exploration of an embeddings space of NCAs through music.</p>
            
            
            <h2>How to Play</h2>
            
            <ul>
            <li><strong>Piano keys</strong>: Click or use keyboard (A-L for white keys, W/E/R/Y/U/O/P for black keys). Each note loads a different latent vector that generates a unique NCA.</li>
            <li><strong>Chords</strong>: Hold multiple keys to blend their latents together, creating hybrid dynamics.</li>
            <li><strong>Instruments</strong>: Use the dropdown to switch between piano, violin, guitar, and more. Each instrument produces distinct visual behaviors.</li>
            <li><strong>Perturbation</strong>: Defaults to max. Press number keys 1-9 to reduce, or 0 for max. Higher values create more chaotic, unpredictable patterns. Press the same key twice to toggle off.</li>
            <li><strong>Random Latent</strong>: Click "Random Latent" to explore completely random points in the learned space.</li>
            </ul>
            
            <h2>Gallery</h2>
            
            <p>Here are some examples of beautiful self-stabilizing patterns found in the space of Neuromusical Cellular Automata. Set the <strong>Perturbation</strong> level high or use to the <strong>Random Latent</strong> button to discover more. You'll see interesting smokey fluid dynamics, shifting colors, and even shapes that appear to be swirling in 3D!</p>
            
            <table class="gallery-table">
            <tr>
            <td><img src="assets/gallery/dancingvines.gif" alt="Dancing Vines\" style="width: 256px; height: 256px;" /></td>
            <td><img src="assets/gallery/eelfire.gif" alt="Eel Fire\" style="width: 256px; height: 256px;" /></td>
            <td><img src="assets/gallery/flowerdolphins.gif" alt="Flower Dolphins\" style="width: 256px; height: 256px;" /></td>
            </tr>
            <tr>
            <td><img src="assets/gallery/lillypads.gif" alt="Lilly Pads\" style="width: 256px; height: 256px;" /></td>
            <td><img src="assets/gallery/organicswirl3D.gif" alt="Organic Swirl 3D\" style="width: 256px; height: 256px;" /></td>
            <td><img src="assets/gallery/paraglider.gif" alt="Paraglider\" style="width: 256px; height: 256px;" /></td>
            </tr>
            <tr>
            <td><img src="assets/gallery/rebirth.gif" alt="Rebirth\" style="width: 256px; height: 256px;" /></td>
            <td><img src="assets/gallery/redgrid.gif" alt="Red Grid\" style="width: 256px; height: 256px;" /></td>
            <td><img src="assets/gallery/water_channels.gif" alt="Water Channels\" style="width: 256px; height: 256px;" /></td>
            </tr>
            </table>
            
            
            <h3>Random NCAs</h3>
            
            <p>In contrast to the above NCAs sampled from a learned latent space (so that we are navigating a particularly interesting and beautiful region of NCA parameter space), most of NCA space is more boring and tends to collapse to blank or static patterns:</p>
            
            <table class="gallery-table">
            <tr>
            <td><img src="assets/random_nca1.gif" alt="rand1\" style="width: 128px; height: 128px;" /></td>
            <td><img src="assets/random_nca2.gif" alt="rand2\" style="width: 128px; height: 128px;" /></td>
            <td><img src="assets/random_nca3.gif" alt="rand3\" style="width: 128px; height: 128px;" /></td>
            </tr>
            </table>
            
            <hr>
            
            <h2>Why This Project</h2>
            
            <p>I have always been fascinated by neural cellular automata (and CAs in general) as they are like little physics engines for tiny virtual universes. So when I started this project I spun up a little NCA editor where I could adjust the architecture and parameter weights of NCAs to explore the space. I found some pretty neat settings, but it turns out to be pretty difficult to find anything interesting by randomly navigating parameter space!</p>
            
            <p>This got me thinking about the embeddings space of NCA parameters, and how we could explore it. To get a smoother and hopefully more interesting embedding space, I decided to train a neural network to <em>generate</em> NCA parameters from a conditioned latent space.</p>
            
            <p>In this way, I've ended up with a neural network (a hypernetwork) that generates neural networks (NCAs) from a latent space, how cool! Being a musician, I eventually came to the idea of constraining the encoder to the space of musical notes from various instruments, which allows us to use music to explore the embedding space, which is very fun.</p>
            
            <h2>Cellular Automata</h2>
            
            <p>Cellular automata are systems of simple cells on a grid, each updating its state based on neighboring values. Classic examples like Conway's Game of Life show how complex global behavior can emerge from purely local rules.</p>
            
            <p>Whereas Conway's Game of Life is the application of one possible update rule, we can think of the space of all possible rules as the set of all state transition functions for a given neighborhood, which could be listed in a (beyond astronomically massive) lookup table. For example a binary CA where the update rule depends on the Moore neighborhood has 2^1024 possible rules (there are 9 cells in a neighborhood + the center cell's new state, which gives us 2^10 (1024) entries per rule). Each entry maps the 9-cell states at time <em>t</em> to the center cell's state at time <em>t+1</em>.</p>
            
            <div class="article-img"><img src="assets/rainbow_gliders.gif" alt="Rainbow Gliders" style="width: 512px; height: 512px;" /></div>
            
            <p><strong>Neural Cellular Automata (NCA)</strong> replace the discrete update rules of CAs with a small neural network, allowing the system to learn its own dynamics from data. A neural network reads local values and outputs updated values for the next time step, which is fed back into the network at time <em>t+1</em>. This makes an NCA effectively a recurrent CNN! NCAs can learn to grow, regenerate, and sustain surprisingly complex dynamical patterns.</p>
            
            <p>The space of all possible NCAs is infinite*, even for a constrained neighborhood, because there are an infinite number of neural network architectures we could apply. For a fixed architecture, we can think of the "embedding space" of all possible parameter values. Typically, this space is pretty sparse and boring, producing mostly noise or blank outputs (although I did randomly stumble upon the "rainbow gliders" shown above - stable little colorful blobs that move!).</p>
            
            <p>*<em>While the number of possible NCA architectures is truly unbounded (we can always add another layer), the number of distinguishable transition functions is finite: the inputs and outputs are encoded with finite-precision floating point. Many different architectures and parameter settings will produce identical behavior. The space is astronomically vast but ultimately discrete, appearing continuous to us.</em></p>
            
            <hr>
            
            <h2>Hardware Constraints</h2>
            
            <p>I landed on the following architecture, which is extremely tiny because I want this to run in realtime in the browser on a CPU, and I needed a fast turn-around time for experiments.</p>
            
            <div class="article-img"><img src="assets/architecture.png" alt="Architecture" /></div>
            
            <h3>Architecture Details</h3>
            
            <p><strong>VAE Encoder</strong> (Context Frames → Latent)</p>
            <ul>
            <li>Input: context frames, 32×32</li>
            <li>Conv layers: 12→32→64→128, each 3×3 kernel, stride 2, BatchNorm, LeakyReLU(0.2)</li>
            <li>Flatten: 128 × 4 × 4 = 2,048 features</li>
            <li>Two linear heads (μ and σ): 2,048 → 64 each</li>
            <li>Output: 64-dimensional latent vector z</li>
            </ul>
            
            <p><strong>HyperNetwork</strong> (Latent → NCA Weights)</p>
            <ul>
            <li>Input: 64-dim latent</li>
            <li>MLP: 64 → 256 → 256 → 4,640, with LayerNorm + ReLU</li>
            <li>Output: weights for 2-layer NCA (2×[16×16×3×3] + 2×[16] = 4,640 params)</li>
            </ul>
            
            <p><strong>NCA</strong> (Grid Evolution)</p>
            <ul>
            <li>Grid: 16 channels (3 RGB visible + 13 hidden state)</li>
            <li>Layer 1: 16→16 channels, 3×3 conv, circular padding, ReLU</li>
            <li>Layer 2: 16→16 channels, 3×3 conv, circular padding, residual add</li>
            <li>Parameters generated per-sample by HyperNetwork</li>
            </ul>
            
            <hr>
            
            <h2>Learned Latent Space</h2>
            
            <p>Instead of exploring the embedding space of random parameter values, we can learn a latent space and use that to <em>generate</em> NCAs. And that's what the Neuromusical Cellular Automata does.</p>
            
            <p>Because this latent space is learned it can be much smoother and more semantically meaningful than exploring the raw space of NCA parameters. Of course, what the latent space actually learns is entirely dependent on the data used to train the model.</p>
            
            <hr>
            
            <h2>Dynamics</h2>
            
            <p>Because NCAs are recurrent, they can learn dynamics and movement. Here is an example of a model trained on sequences of frames from dynamic simulations. Given a few context frames as input, the model learns to predict how the system evolves over time. Each training sequence captures a different behavior, forcing the model to internalize the underlying rules of motion rather than memorizing individual frames.</p>
            
            <p>The ground truth is shown on the right with the model predictions on the left:</p>
            
            <div class="article-img"><img src="assets/groundtruth-1.gif" alt="Ground Truth Example 1" style="width: 512px; height: 256px;" /></div>
            <div class="article-img"><img src="assets/groundtruth-2.gif" alt="Ground Truth Example 2" style="width: 512px; height: 256px;" /></div>
            <div class="article-img"><img src="assets/groundtruth-3.gif" alt="Ground Truth Example 3" style="width: 512px; height: 256px;" /></div>
            
            <p>As you can see, the NCA does capture the general size, color, and motion of the objects, although it tends to blur over time as it struggles to guess the exact next frame. This is a classic problem for sequence prediction in continuous space, and could probably be rectictfied with diffusion or other techniques. But remember this is a <em>tiny</em> little network because it needs to run in realtime on a CPU and train in reasonable time. And anyway, I kind of like how the objects smear out over time and generate interesting patterns - it makes the latent space more surprising.</p>
            
            <hr>
            
            <h2>Music</h2>
            
            <p>The training data for the Neuromusical Cellular Automata and the demo at the top of this page is generated by pairing audio with deterministic visualizations.</p>
            
            <div class="article-img"><img src="assets/spectrogram_piano.gif" alt="Ground Truth Example 1" style="width: 768px; height: 256px;" /></div>
            
            <p>For each instrument sample (piano, violin, etc.), we extract audio features: detected harmonics, loudness (RMS), and spectral brightness. These features drive the "circles" visualization where each harmonic becomes a colored circle - low frequencies appear warm (red/orange) near the center, high frequencies appear cool (blue) toward the edges. Circle size pulses with loudness, and positions orbit based on harmonic relationships.</p>
            
            <div class="article-img"><img src="assets/spectrogram_xylophone.gif" alt="Ground Truth Example 2" style="width: 768px; height: 256px;" /></div>
            
            <p>The first frame of each training sequence is a <strong>mel spectrogram</strong> (a 2D image of frequency vs. time), which the encoder compresses into a 64-dimensional latent vector. The decoder, a hypernetwork, transforms this latent into NCA weights that generate the subsequent circle animation frames.</p>
            
            <div class="article-img"><img src="assets/spectrogram_clarinet.gif" alt="Ground Truth Example 3" style="width: 768px; height: 256px;" /></div>
            
            <p>For the piano interface, we pre-generate spectrograms for all 19 notes (F4 through B5) across every instrument. Each spectrogram is encoded into its latent vector and stored in a manifest. When you press a piano key, the corresponding latent is loaded; pressing multiple keys (a chord) averages their latents together. This blended latent then generates the NCA in real-time, creating visualizations that interpolate between the learned behaviors of each note.</p>
            
            <hr>
            
            <h2>Realtime Exploration</h2>
            
            <p>This demo combines two separately trained models:</p>
            
            <ul>
            <li><strong>Encoder:</strong> From the spectrogram-to-NCA model used in the piano demo above</li>
            <li><strong>Hypernetwork:</strong> From a new model trained to generate emoji (or rather to <em>generate NCAs</em> which generate emoji)</li>
            </ul>
            
            <p>The idea is to explore whether we can use music and sound to navigate the space of possible NCAs, even when the hypernetwork wasn't jointly trained with spectrograms. Your microphone audio gets converted to a spectrogram, encoded to a latent vector, and that latent drives the emoji NCA's dynamics in real-time.</p>
            
            <p>
        <div class="rt-section" id="realtimeAudioSection">
            <p class="rt-subtitle">Try making sounds with different textures: humming, clapping, whistling, or playing different instruments. Each sound creates a unique spectrogram that drives the NCA dynamics.</p>

            <div class="rt-container">
                <div class="rt-displays">
                    <div class="rt-display">
                        <canvas id="rtSpecCanvas" width="256" height="256"></canvas>
                        <div class="label">Live Spectrogram</div>
                    </div>
                    <div class="rt-display">
                        <canvas id="rtNcaCanvas" width="256" height="256"></canvas>
                        <div class="label">NCA Output</div>
                    </div>
                </div>

                <div class="rt-controls">
                    <button id="rtEnableMicBtn">Enable Microphone</button>
                    <span id="rtMicStatus"></span>
                </div>
                <div class="rt-controls" style="margin-top: 10px;">
                    <label for="rtSampleRateSlider" style="color: #aaa; font-size: 0.9em;">Sample Rate:</label>
                    <input type="range" id="rtSampleRateSlider" min="100" max="3000" value="1500" style="width: 120px;">
                    <span id="rtSampleRateValue" style="color: #4fc3f7; font-family: monospace; min-width: 50px;">1500ms</span>
                </div>
            </div>
        </div>
</p>
            
            <p>With enough experimentation it's possible to find recognizable emoji in the above demo. The spectrogram encoder's latent space doesn't match what the emoji hypernetwork expects, but I trained a simple MLP to project between the manifolds of the two latent spaces. I'd like to experiment with different methods to make the projection more obviously meaningful (but we'd likely need networks powerful enough to actually <em>represent</em> that meaning).</p>
            
            <p>Still, what makes this interesting is that we're navigating a structured region of NCA parameter space. The hypernetwork constrains the outputs to "plausible" cellular automata, and the audio encoder provides a consistent way to explore that space. Different sounds produce different dynamics, and you <em>can</em> discover regularities.</p>
            
            
            <p>For reference, here's what the hypernetwork (and its generated NCAs) produces with its matching encoder:</p>
            
            <div class="article-img"><img src="assets/emoji1.gif" alt="Emoji NCA 1\" style="width: 256px; height: 128px;" /></div>
            <div class="article-img"><img src="assets/emoji2.gif" alt="Emoji NCA 2\" style="width: 256px; height: 128px;" /></div>
            <div class="article-img"><img src="assets/emoji3.gif" alt="Emoji NCA 3\" style="width: 256px; height: 128px;" /></div>
            
            <hr>
            
            <h2>Future Potential</h2>
            
            <p>This approach opens several interesting directions:</p>
            
            <ul>
            <li>The latent space could be conditioned on higher-level concepts, allowing semantic control over the generated dynamics</li>
            <li>Larger grids and deeper NCAs, and more interesting training data could capture more complex phenomena</li>
            <li>The ability for NCAs to track/produce <strong>agentic behavior</strong> on the grid is particularly fascinating</li>
            <li>Visualizing and analyzing the <strong>hidden channels</strong> of the NCA grid to find out more about how the visible RGB channels are computed, which would be especially interesing in the case of agentic behavior</li>
            </ul>
            
            <h3>Agentic behavior</h3>
            <p>I did some early experiments with boids exhibiting various behaviors such as flocking or predator-prey dynamics, but this proved too challenging for my tiny 2-layer conv nets. It would be fascinating to see if an NCA could learn theory of mind to generate the behaviors of simple agents. A more ambitious goal would be to model human player actions in Atari games, or even more complex environments and life-like behaviors from nature itself.</p>
            
            <h3>Sound as Navigation</h3>
            <p>Broadly, music offers a compelling interface for exploring high-dimensional latent spaces. The simple visualizations used here map audio features to color and motion, but richer correspondences are possible. We could learn a deeper semantic structure: tension and resolution in a symphony, the texture of a jazz improvisation, or the chaos of city and forest soundscapes all provide meaningful ways of steering through latent space.</p>
            
            <h3>Open Ended</h3>
            <p>The combination of neural cellular automata with learned latent spaces suggests a fascinating paradigm: compact, local update rules that are themselves generated by a learned model, producing an open-ended family of emergent systems from a single trained network.</p>
            
            <hr>
            
            <h2>Bonus: Game of Life</h2>
            
            <p>As an experiment, I trained the same NCA architecture on Conway's Game of Life—a cellular automaton that is famously Turing complete. The NCA successfully learned to emulate the Game of Life rules, correctly simulating gliders, oscillators, and chaotic patterns.</p>
            
            <div class="article-img"><img src="assets/gol2.gif" alt="GoL Example 2" style="width: 512px; height: 256px;" /></div>
            <div class="article-img"><img src="assets/gol3.gif" alt="GoL Example 3" style="width: 512px; height: 256px;" /></div>
            
            <p><em>Left: NCA prediction. Right: Ground truth.</em></p>
            
            <p>This result demonstrates that our tiny 2-layer NCA architecture has sufficient expressive power to encode the complete Game of Life transition function. Since the Game of Life is Turing complete, this means our NCA setup is capable of universal computation, and it's <em>learnable</em>, neat!</p>
            
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/tone@14/build/Tone.js"></script>
    
    <script>
    (function() {
    "use strict";

    // -----------------------------------------------------------------------
    // Config (filled from manifest at runtime)
    // -----------------------------------------------------------------------
    const GITHUB_PAGES = true;
    let width = 0, height = 0, channels = 0;
    let contextFramesCount = 0, numSteps = 0;
    let latentDim = 0, gridChannels = 0;
    let numSequences = 0;
    let manifest = null;

    // -----------------------------------------------------------------------
    // Runtime state
    // -----------------------------------------------------------------------
    let decodeSession = null, ncaStepSession = null;
    let currentSeqIdx = 0, currentFrameIdx = 0;
    let isPlaying = true;
    let playbackSpeed = 1.0;
    let currentMode = GITHUB_PAGES ? 'latent' : 'sequence';
    let isLoading = false;  // true while selectSequence/decodeLatent in progress
    let noiseLevel = 1.0;   // default to max; keys 1-9 map to 0.1-0.9, key 0 = 1.0

    // Current latent & decoded params
    let currentZ = null;           // Float32Array [latentDim]
    let cachedParams = null;       // { layer1_w, layer1_b, layer2_w, layer2_b } ort.Tensor
    let currentNcaFrame = null;    // Float32Array [C*H*W] in [0,1] (for display)
    let currentHidden = null;      // Float32Array [(gridChannels-C)*H*W] evolving hidden channels

    // Ground truth data for current sequence
    let gtData = null;             // Uint8Array [T*H*W*C]
    let gtFrameCount = 0;

    // Slot embeddings (piano) - loaded from instrument_embeddings
    const slotEmbeddings = new Array(19).fill(null);   // array | null
    const slotActive = new Array(19).fill(false);      // which slots are pressed
    let currentInstrument = 'piano';

    // Canvas refs
    const gtCanvas = GITHUB_PAGES ? null : document.getElementById('gtCanvas');
    const ncaCanvas = document.getElementById('ncaCanvas');
    const gtCtx = gtCanvas ? gtCanvas.getContext('2d') : null;
    const ncaCtx = ncaCanvas.getContext('2d');
    let contextCanvases = [];

    // -----------------------------------------------------------------------
    // Utilities
    // -----------------------------------------------------------------------
    function randn() {
        const u1 = Math.random(), u2 = Math.random();
        return Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);
    }

    function randnArray(n) {
        const arr = new Float32Array(n);
        for (let i = 0; i < n; i++) arr[i] = randn();
        return arr;
    }

    function sigmoid(x) {
        return 1 / (1 + Math.exp(-x));
    }

    // -----------------------------------------------------------------------
    // Rendering
    // -----------------------------------------------------------------------
    function renderFrame(ctx, canvas, pixels, w, h, c) {
        const displayW = canvas.width;
        const displayH = canvas.height;
        const imageData = ctx.createImageData(displayW, displayH);
        const scaleX = displayW / w;
        const scaleY = displayH / h;

        for (let y = 0; y < displayH; y++) {
            for (let x = 0; x < displayW; x++) {
                const srcX = Math.floor(x / scaleX);
                const srcY = Math.floor(y / scaleY);
                const srcIdx = (srcY * w + srcX) * c;
                const dstIdx = (y * displayW + x) * 4;

                if (c === 3) {
                    imageData.data[dstIdx]     = pixels[srcIdx];
                    imageData.data[dstIdx + 1] = pixels[srcIdx + 1];
                    imageData.data[dstIdx + 2] = pixels[srcIdx + 2];
                } else {
                    imageData.data[dstIdx]     = pixels[srcIdx];
                    imageData.data[dstIdx + 1] = pixels[srcIdx];
                    imageData.data[dstIdx + 2] = pixels[srcIdx];
                }
                imageData.data[dstIdx + 3] = 255;
            }
        }
        ctx.putImageData(imageData, 0, 0);
    }

    function renderNcaCanvas() {
        if (!currentNcaFrame) return;
        // currentNcaFrame: float32 [C, H, W] in CHW order, values [0,1]
        const pixels = new Uint8Array(height * width * channels);
        for (let ch = 0; ch < channels; ch++) {
            for (let y = 0; y < height; y++) {
                for (let x = 0; x < width; x++) {
                    const srcIdx = ch * height * width + y * width + x;
                    const dstIdx = (y * width + x) * channels + ch;
                    pixels[dstIdx] = Math.min(255, Math.max(0, Math.round(currentNcaFrame[srcIdx] * 255)));
                }
            }
        }
        renderFrame(ncaCtx, ncaCanvas, pixels, width, height, channels);
    }

    function renderGtCanvas() {
        if (GITHUB_PAGES || !gtData || !gtCtx) return;
        const frameSize = height * width * channels;
        const totalFrames = Math.floor(gtData.length / frameSize);
        // Clamp to last available frame instead of showing black
        const frameIdx = Math.min(contextFramesCount + currentFrameIdx, totalFrames - 1);
        const offset = frameIdx * frameSize;
        const pixels = gtData.subarray(offset, offset + frameSize);
        renderFrame(gtCtx, gtCanvas, pixels, width, height, channels);
    }

    function renderContextFrames() {
        if (GITHUB_PAGES || !gtData) return;
        for (let i = 0; i < contextFramesCount && i < contextCanvases.length; i++) {
            const offset = i * height * width * channels;
            const end = offset + height * width * channels;
            if (end > gtData.length) break;
            const pixels = gtData.subarray(offset, end);
            const canvas = contextCanvases[i];
            const cctx = canvas.getContext('2d');
            renderFrame(cctx, canvas, pixels, width, height, channels);
        }
    }

    function createContextCanvases() {
        if (GITHUB_PAGES) return;
        const container = document.getElementById('contextFrames');
        container.innerHTML = '';
        contextCanvases = [];

        for (let i = 0; i < contextFramesCount; i++) {
            const div = document.createElement('div');
            div.className = 'context-frame';

            const canvas = document.createElement('canvas');
            canvas.width = 64;
            canvas.height = 64;
            canvas.style.cssText = 'border: 1px solid #4fc3f7; image-rendering: pixelated;';

            const label = document.createElement('span');
            label.textContent = 't-' + (contextFramesCount - 1 - i);

            div.appendChild(canvas);
            div.appendChild(label);
            container.appendChild(div);
            contextCanvases.push(canvas);
        }
    }

    // -----------------------------------------------------------------------
    // ONNX inference
    // -----------------------------------------------------------------------
    async function decodeLatent(z) {
        const zTensor = new ort.Tensor('float32', z, [1, latentDim]);
        const results = await decodeSession.run({ z: zTensor });

        cachedParams = {
            layer1_w: results.layer1_w,
            layer1_b: results.layer1_b,
            layer2_w: results.layer2_w,
            layer2_b: results.layer2_b,
        };

        // first_frame is full grid [1, gridChannels, H, W] from FirstFrameDecoder
        // RGB channels are already sigmoided, hidden channels are tanh * 0.1
        const initialGrid = results.first_frame.data;
        const gridSize = gridChannels * height * width;
        const imgSize = channels * height * width;
        const hiddenSize = gridSize - imgSize;

        // Initialize current state from FirstFrameDecoder output
        currentNcaFrame = new Float32Array(imgSize);
        currentHidden = new Float32Array(hiddenSize);
        for (let i = 0; i < imgSize; i++) {
            currentNcaFrame[i] = initialGrid[i];
        }
        for (let i = 0; i < hiddenSize; i++) {
            currentHidden[i] = initialGrid[imgSize + i];
        }

        // Run NCA steps, squashing after each (matches training)
        for (let s = 0; s < numSteps; s++) {
            const grid = new Float32Array(gridSize);
            grid.set(currentNcaFrame);
            grid.set(currentHidden, imgSize);

            const gridTensor = new ort.Tensor('float32', grid, [1, gridChannels, height, width]);
            const out = await ncaStepSession.run({
                grid: gridTensor,
                layer1_w: cachedParams.layer1_w,
                layer1_b: cachedParams.layer1_b,
                layer2_w: cachedParams.layer2_w,
                layer2_b: cachedParams.layer2_b,
            });

            // Squash after each step
            const raw = out.new_grid.data;
            for (let i = 0; i < imgSize; i++) {
                currentNcaFrame[i] = sigmoid(raw[i]);
            }
            for (let i = 0; i < hiddenSize; i++) {
                currentHidden[i] = Math.tanh(raw[imgSize + i]);
            }
        }
    }

    async function stepNca() {
        // Run ONE NCA step (animation loop shows every step)
        const gridSize = gridChannels * height * width;
        const imgSize = channels * height * width;
        const hiddenSize = gridSize - imgSize;

        // Build grid from current state
        const grid = new Float32Array(gridSize);
        grid.set(currentNcaFrame);
        grid.set(currentHidden, imgSize);

        // Run one NCA step
        const gridTensor = new ort.Tensor('float32', grid, [1, gridChannels, height, width]);
        const out = await ncaStepSession.run({
            grid: gridTensor,
            layer1_w: cachedParams.layer1_w,
            layer1_b: cachedParams.layer1_b,
            layer2_w: cachedParams.layer2_w,
            layer2_b: cachedParams.layer2_b,
        });

        // Squash (matches training)
        const raw = out.new_grid.data;
        for (let i = 0; i < imgSize; i++) {
            currentNcaFrame[i] = sigmoid(raw[i]);
        }
        for (let i = 0; i < hiddenSize; i++) {
            currentHidden[i] = Math.tanh(raw[imgSize + i]);
        }
    }

    // -----------------------------------------------------------------------
    // Sequence & latent management
    // -----------------------------------------------------------------------
    async function selectSequence(idx) {
        if (GITHUB_PAGES) return;  // No sequences in GitHub Pages mode
        isLoading = true;
        currentSeqIdx = ((idx % numSequences) + numSequences) % numSequences;
        currentFrameIdx = 0;

        // Load GT binary
        const padded = String(currentSeqIdx).padStart(3, '0');
        const resp = await fetch('data/seq_' + padded + '.bin');
        gtData = new Uint8Array(await resp.arrayBuffer());
        gtFrameCount = manifest.sequences[currentSeqIdx].frames;

        // Pre-encoded latent
        currentZ = new Float32Array(manifest.sequences[currentSeqIdx].z);
        await decodeLatent(currentZ);

        document.getElementById('seqNum').textContent = currentSeqIdx;
        document.getElementById('latentSource').textContent = 'encoded';

        renderContextFrames();
        isLoading = false;
    }

    function resetNca() {
        // Re-decode from currentZ to reset first frame
        decodeLatent(currentZ);
        currentFrameIdx = 0;
    }

    let lastPressedSlot = -1;  // Track last pressed key for spectrogram display

    async function randomLatent() {
        currentZ = randnArray(latentDim);
        await decodeLatent(currentZ);
        currentFrameIdx = 0;
        document.getElementById('latentSource').textContent = 'random';
        lastPressedSlot = -1;
        document.getElementById('specImg').src = '';
    }

    const NOTE_ORDER = ['F4', 'Fs4', 'G4', 'Gs4', 'A4', 'As4', 'B4',
                        'C5', 'Cs5', 'D5', 'Ds5', 'E5', 'F5', 'Fs5',
                        'G5', 'Gs5', 'A5', 'As5', 'B5'];

    function updateSpectrogramDisplay() {
        const specImg = document.getElementById('specImg');
        if (currentMode !== 'latent' || lastPressedSlot < 0) {
            specImg.src = '';
            return;
        }
        const note = NOTE_ORDER[lastPressedSlot];
        specImg.src = `spectrograms/${currentInstrument}/${note}.png`;
    }

    async function applySlotSelection() {
        // Collect active slots with embeddings
        const activeSlots = [];
        const activeIndices = [];
        for (let i = 0; i < 19; i++) {
            if (slotActive[i] && slotEmbeddings[i]) {
                activeSlots.push(slotEmbeddings[i]);
                activeIndices.push(i);
            }
        }
        if (activeSlots.length === 0) {
            return;
        }
        // Track last pressed for spectrogram display
        if (activeIndices.length > 0) {
            lastPressedSlot = activeIndices[activeIndices.length - 1];
            updateSpectrogramDisplay();
        }
        // Average embeddings for chord
        const mean = new Float32Array(activeSlots[0].length).fill(0);
        for (const emb of activeSlots) {
            for (let j = 0; j < emb.length; j++) {
                mean[j] += emb[j] / activeSlots.length;
            }
        }
        // Apply noise based on number key selection
        if (noiseLevel > 0) {
            for (let i = 0; i < mean.length; i++) mean[i] += randn() * noiseLevel;
        }
        currentZ = mean;
        await decodeLatent(currentZ);
        currentFrameIdx = 0;
    }

    // -----------------------------------------------------------------------
    // Instrument embeddings
    // -----------------------------------------------------------------------
    function loadInstrumentEmbeddings(instrumentName) {
        const embeddings = manifest.instrument_embeddings[instrumentName];
        if (!embeddings) {
            console.warn('No embeddings for instrument: ' + instrumentName);
            for (let i = 0; i < 19; i++) slotEmbeddings[i] = null;
            updateSlotUI();
            return;
        }
        for (let i = 0; i < 19; i++) {
            const note = NOTE_ORDER[i];
            slotEmbeddings[i] = embeddings[note] ? new Float32Array(embeddings[note]) : null;
        }
        updateSlotUI();
    }

    function getAvailableInstruments() {
        if (!manifest.instrument_embeddings) return [];
        return Object.keys(manifest.instrument_embeddings).sort();
    }

    // -----------------------------------------------------------------------
    // Piano keyboard
    // -----------------------------------------------------------------------
    const NOTES = [
        { note: 'F',  tone: 'F4',  key: 'a', type: 'white' },
        { note: 'F#', tone: 'F#4', key: 'w', type: 'black' },
        { note: 'G',  tone: 'G4',  key: 's', type: 'white' },
        { note: 'G#', tone: 'G#4', key: 'e', type: 'black' },
        { note: 'A',  tone: 'A4',  key: 'd', type: 'white' },
        { note: 'A#', tone: 'A#4', key: 'r', type: 'black' },
        { note: 'B',  tone: 'B4',  key: 'f', type: 'white' },
        { note: 'C',  tone: 'C5',  key: 'g', type: 'white' },
        { note: 'C#', tone: 'C#5', key: 'y', type: 'black' },
        { note: 'D',  tone: 'D5',  key: 'h', type: 'white' },
        { note: 'D#', tone: 'D#5', key: 'u', type: 'black' },
        { note: 'E',  tone: 'E5',  key: 'j', type: 'white' },
        { note: 'F',  tone: 'F5',  key: 'k', type: 'white' },
        { note: 'F#', tone: 'F#5', key: 'o', type: 'black' },
        { note: 'G',  tone: 'G5',  key: 'l', type: 'white' },
        { note: 'G#', tone: 'G#5', key: 'p', type: 'black' },
        { note: 'A',  tone: 'A5',  key: ';', type: 'white' },
        { note: 'A#', tone: 'A#5', key: '[', type: 'black' },
        { note: 'B',  tone: 'B5',  key: "'", type: 'white' },
    ];
    const keyToSlot = {};
    NOTES.forEach((n, i) => { keyToSlot[n.key] = i; });

    // -----------------------------------------------------------------------
    // Instrument samples (Tone.js Sampler)
    // -----------------------------------------------------------------------
    const SAMPLE_BASE = 'https://nbrosowsky.github.io/tonejs-instruments/samples/';

    // Sharp-to-filename: F#4 -> Fs4.mp3
    function noteToFile(n) {
        return n.replace('#', 's') + '.mp3';
    }

    // Per-instrument sample note sets (subset — Tone.Sampler pitch-shifts to fill gaps)
    const INSTRUMENT_SAMPLES = {
        'piano':            ['A1','A2','A3','A4','A5','A6','C1','C2','C3','C4','C5','C6','D#1','D#2','D#3','D#4','D#5','D#6','F#1','F#2','F#3','F#4','F#5','F#6'],
        'bass-electric':    ['A#1','A#2','A#3','A#4','C#1','C#2','C#3','C#4','E1','E2','E3','E4','G1','G2','G3','G4'],
        'bassoon':          ['A2','A3','A4','C3','C4','C5','E4','G2','G3','G4'],
        'cello':            ['A2','A3','A4','C2','C3','C4','C5','D#2','D#3','D#4','F#3','F#4','G#2','G#3','G#4'],
        'clarinet':         ['A#3','A#4','A#5','D3','D4','D5','D6','F3','F4','F5','F#6'],
        'contrabass':       ['A2','A#1','B3','C2','C#3','D2','E2','E3','F#1','F#2','G1','G#2','G#3'],
        'flute':            ['A4','A5','A6','C4','C5','C6','C7','E4','E5','E6'],
        'french-horn':      ['A1','A3','C2','C4','D3','D5','D#2','F3','F5','G2'],
        'guitar-acoustic':  ['A2','A3','A4','C3','C4','C5','D#2','D#3','E2','E3','E4','F#2','F#3','F#4','G#2','G#3','G#4'],
        'guitar-electric':  ['A2','A3','A4','A5','C3','C4','C5','C6','C#2','D#3','D#4','D#5','E2','F#2','F#3','F#4','F#5'],
        'guitar-nylon':     ['A2','A3','A4','A5','B1','B2','B3','B4','C#3','C#4','C#5','D2','D3','D5','E2','E3','E4','E5','F#2','F#3','F#4','F#5','G3'],
        'harmonium':        ['A2','A3','A4','C2','C3','C4','C5','D#2','D#3','D#4','F#2','F#3','G#2','G#3','G#4'],
        'harp':             ['A2','A4','A6','B1','B3','B5','C3','C5','D2','D4','D6','E1','E3','E5','F2','F4','F6','G1','G3','G5'],
        'organ':            ['A1','A2','A3','A4','A5','C1','C2','C3','C4','C5','C6','D#1','D#2','D#3','D#4','D#5','F#1','F#2','F#3','F#4','F#5'],
        'saxophone':        ['A4','A5','A#3','A#4','C4','C5','C#3','D3','D4','D5','D#3','D#4','E3','E4','E5','F3','F4','F5','F#3','F#4','G3','G4','G5','G#3','G#4'],
        'trombone':         ['A#1','A#2','A#3','C3','C4','C#2','C#4','D3','D4','D#2','D#3','D#4','F2','F3','F4','G#2','G#3'],
        'trumpet':          ['A3','A5','A#4','C4','C6','D5','D#4','F3','F4','F5','G4'],
        'tuba':             ['A#1','A#2','A#3','D3','D4','D#2','F1','F2','F3'],
        'violin':           ['A3','A4','A5','A6','C4','C5','C6','E4','E5','E6','G4','G5','G6'],
        'xylophone':        ['C5','C6','C7','C8','G4','G5','G6','G7'],
    };

    const INSTRUMENT_NAMES = Object.keys(INSTRUMENT_SAMPLES).sort();

    let currentSampler = null;
    let samplerReady = false;

    function loadInstrument(name) {
        samplerReady = false;
        const statusEl = document.getElementById('instrumentStatus');
        statusEl.textContent = 'Loading...';

        if (currentSampler) {
            currentSampler.dispose();
            currentSampler = null;
        }

        const notes = INSTRUMENT_SAMPLES[name];
        const urls = {};
        notes.forEach(n => { urls[n] = noteToFile(n); });

        currentSampler = new Tone.Sampler({
            urls: urls,
            baseUrl: SAMPLE_BASE + name + '/',
            onload: () => {
                samplerReady = true;
                statusEl.textContent = '';
            },
            onerror: () => {
                statusEl.textContent = 'Failed to load';
            },
        }).toDestination();
    }

    // Instrument dropdown - populated from manifest.instrument_embeddings in init()
    const instrumentSelect = document.getElementById('instrumentSelect');

    function populateInstrumentDropdown() {
        instrumentSelect.innerHTML = '';
        const instruments = getAvailableInstruments();
        instruments.forEach(name => {
            const opt = document.createElement('option');
            opt.value = name;
            opt.textContent = name;
            instrumentSelect.appendChild(opt);
        });
        // Default to piano if available, otherwise first instrument
        if (instruments.includes('piano')) {
            instrumentSelect.value = 'piano';
            currentInstrument = 'piano';
        } else if (instruments.length > 0) {
            instrumentSelect.value = instruments[0];
            currentInstrument = instruments[0];
        }
    }

    instrumentSelect.addEventListener('change', (e) => {
        currentInstrument = e.target.value;
        loadInstrumentEmbeddings(currentInstrument);
        loadInstrument(currentInstrument);
        updateSpectrogramDisplay();
    });

    // Audio helpers using Tone.Sampler
    const activeNotes = {};  // slot index -> tone name
    function startTone(idx) {
        if (!samplerReady || activeNotes[idx]) return;
        const noteName = NOTES[idx].tone;
        activeNotes[idx] = noteName;
        Tone.start();
        currentSampler.triggerAttack(noteName);
    }
    function stopTone(idx) {
        if (!activeNotes[idx]) return;
        const noteName = activeNotes[idx];
        delete activeNotes[idx];
        if (currentSampler) currentSampler.triggerRelease(noteName);
    }
    function playTone(idx) {
        if (!samplerReady) return;
        Tone.start();
        currentSampler.triggerAttackRelease(NOTES[idx].tone, 0.15);
    }

    const pianoKeys = [];
    const pianoContainer = document.getElementById('pianoKeyboard');

    for (let i = 0; i < NOTES.length; i++) {
        const key = document.createElement('div');
        key.className = 'piano-key ' + NOTES[i].type;
        const keyHint = document.createElement('div');
        keyHint.className = 'key-hint';
        keyHint.textContent = NOTES[i].key;
        key.appendChild(keyHint);

        key.addEventListener('click', () => {
            playTone(i);
            if (slotEmbeddings[i]) {
                slotActive[i] = !slotActive[i];
                applySlotSelection();
                updateSlotUI();
            }
        });

        pianoContainer.appendChild(key);
        pianoKeys.push(key);
    }

    // Keyboard shortcuts
    // Number keys 1-9,0 set noise level
    const noiseKeyMap = { '1': 0.1, '2': 0.2, '3': 0.3, '4': 0.4, '5': 0.5, '6': 0.6, '7': 0.7, '8': 0.8, '9': 0.9, '0': 1.0 };

    function updateNoiseMeter() {
        document.getElementById('noiseMeterFill').style.height = (noiseLevel * 100) + '%';
        document.getElementById('noiseMeterValue').textContent = noiseLevel > 0 ? noiseLevel.toFixed(1) : '0';
        document.getElementById('latentSource').textContent =
            noiseLevel > 0 ? 'perturb: ' + noiseLevel.toFixed(1) : 'encoded';
    }

    document.addEventListener('keydown', (e) => {
        if (e.target.tagName === 'INPUT' || e.target.tagName === 'TEXTAREA') return;
        if (e.repeat) return;

        // Number keys toggle noise level
        if (e.key in noiseKeyMap) {
            const level = noiseKeyMap[e.key];
            noiseLevel = (noiseLevel === level) ? 0 : level;
            updateNoiseMeter();
            return;
        }

        const idx = keyToSlot[e.key];
        if (idx !== undefined && slotEmbeddings[idx]) {
            startTone(idx);
            slotActive[idx] = true;
            applySlotSelection();
            updateSlotUI();
        }
    });
    document.addEventListener('keyup', (e) => {
        if (e.target.tagName === 'INPUT' || e.target.tagName === 'TEXTAREA') return;
        const idx = keyToSlot[e.key];
        if (idx !== undefined) {
            stopTone(idx);
            slotActive[idx] = false;
            // Check if any slots still active
            const anyActive = slotActive.some(v => v);
            if (anyActive) applySlotSelection();
            updateSlotUI();
        }
    });

    function updateSlotUI() {
        for (let i = 0; i < NOTES.length; i++) {
            const hasFilled = slotEmbeddings[i] !== null;
            const isSelected = slotActive[i];
            pianoKeys[i].className = 'piano-key ' + NOTES[i].type
                + (hasFilled ? ' filled' : '')
                + (isSelected ? ' selected' : '');
            pianoKeys[i].title = hasFilled ? NOTE_ORDER[i] : '';
        }
        // Update latent source display
        const activeIndices = slotActive.map((v, i) => v ? i : -1).filter(i => i >= 0);
        if (activeIndices.length > 1) {
            document.getElementById('latentSource').textContent =
                activeIndices.map(i => NOTES[i].note).join('+') + ' (chord)';
        } else if (activeIndices.length === 1) {
            document.getElementById('latentSource').textContent = NOTES[activeIndices[0]].note;
        }
    }

    // -----------------------------------------------------------------------
    // Mode switching
    // -----------------------------------------------------------------------
    function setMode(mode) {
        currentMode = mode;
        const isSeq = mode === 'sequence';
        document.getElementById('gtViewer').style.display = isSeq ? '' : 'none';
        document.getElementById('specViewer').style.display = isSeq ? 'none' : '';
        document.getElementById('contextSection').style.display = isSeq ? '' : 'none';
        document.getElementById('seqControls').style.display = isSeq ? '' : 'none';
        document.getElementById('latentControls').style.display = isSeq ? 'none' : '';
        document.getElementById('seqInfo').style.display = isSeq ? '' : 'none';
        document.getElementById('seqModeBtn').classList.toggle('active-mode', isSeq);
        document.getElementById('latentModeBtn').classList.toggle('active-mode', !isSeq);
        if (!isSeq) {
            randomLatent();
        } else {
            selectSequence(currentSeqIdx);
        }
    }

    if (!GITHUB_PAGES) {
        document.getElementById('seqModeBtn').addEventListener('click', () => setMode('sequence'));
        document.getElementById('latentModeBtn').addEventListener('click', () => setMode('latent'));
    }

    // -----------------------------------------------------------------------
    // Controls
    // -----------------------------------------------------------------------
    document.getElementById('playPauseBtn').addEventListener('click', () => {
        isPlaying = !isPlaying;
        document.getElementById('playPauseBtn').textContent = isPlaying ? 'Pause' : 'Play';
    });

    document.getElementById('stepBtn').addEventListener('click', async () => {
        await stepNca();
        currentFrameIdx++;
        document.getElementById('frameNum').textContent = currentFrameIdx;
        renderNcaCanvas();
        if (currentMode === 'sequence') renderGtCanvas();
    });

    document.getElementById('resetBtn').addEventListener('click', () => {
        resetNca();
        currentFrameIdx = 0;
        document.getElementById('frameNum').textContent = 0;
    });

    if (!GITHUB_PAGES) {
        document.getElementById('prevSeqBtn').addEventListener('click', () => {
            selectSequence(currentSeqIdx - 1);
        });

        document.getElementById('nextSeqBtn').addEventListener('click', () => {
            selectSequence(currentSeqIdx + 1);
        });

        document.getElementById('randomSeqBtn').addEventListener('click', () => {
            selectSequence(Math.floor(Math.random() * numSequences));
        });
    }

    document.getElementById('randomLatentBtn').addEventListener('click', () => {
        randomLatent();
    });

    // -----------------------------------------------------------------------
    // GIF recording (not available in GitHub Pages mode)
    // -----------------------------------------------------------------------
    let gifEncoder = null;
    let isRecording = false;
    let gifIncludeSpectrogram = false;
    const gifSize = 256;

    function captureGifFrame() {
        // No-op in GitHub Pages mode or when not recording
        if (GITHUB_PAGES || !isRecording || !gifEncoder) return;
    }

    if (!GITHUB_PAGES) {
        const recordBtn = document.getElementById('recordBtn');
        let gifWorkerBlob = null;

        async function ensureGifWorker() {
            if (gifWorkerBlob) return gifWorkerBlob;
            const resp = await fetch('https://cdn.jsdelivr.net/npm/gif.js@0.2.0/dist/gif.worker.js');
            const text = await resp.text();
            const blob = new Blob([text], { type: 'application/javascript' });
            gifWorkerBlob = URL.createObjectURL(blob);
            return gifWorkerBlob;
        }

        function hasSpectrogram() {
            const specImg = document.getElementById('specImg');
            const hasSrc = specImg.src && specImg.src.includes('spectrograms/');
            return currentMode === 'latent' && lastPressedSlot >= 0 && hasSrc;
        }

        async function startRecording() {
            await decodeLatent(currentZ);
            currentFrameIdx = 0;

            const workerScript = await ensureGifWorker();
            gifIncludeSpectrogram = hasSpectrogram();
            // Sequence mode: context + NCA + GT (3 panels)
            // Spectrogram mode: spec + NCA (2 panels)
            // Free mode: NCA only (1 panel)
            let gifWidth = gifSize;
            if (currentMode === 'sequence') {
                gifWidth = gifSize * 3;  // context | NCA | GT
            } else if (gifIncludeSpectrogram) {
                gifWidth = gifSize * 2;
            }
            gifEncoder = new GIF({
                workers: 2,
                quality: 10,
                width: gifWidth,
                height: gifSize,
                workerScript: workerScript,
            });
            isRecording = true;
            recordBtn.textContent = 'Stop Recording';
            recordBtn.style.background = '#f44336';
        }

        captureGifFrame = function() {
            if (!isRecording || !gifEncoder) return;
            const offscreen = document.createElement('canvas');
            let gifWidth = gifSize;
            if (currentMode === 'sequence') {
                gifWidth = gifSize * 3;
            } else if (gifIncludeSpectrogram) {
                gifWidth = gifSize * 2;
            }
            offscreen.width = gifWidth;
            offscreen.height = gifSize;
            const octx = offscreen.getContext('2d');

            if (currentMode === 'sequence') {
                // Draw last context frame on the left
                const lastContextCanvas = contextCanvases[contextCanvases.length - 1];
                if (lastContextCanvas) {
                    octx.drawImage(lastContextCanvas, 0, 0, gifSize, gifSize);
                }
                octx.drawImage(ncaCanvas, gifSize, 0, gifSize, gifSize);
                octx.drawImage(gtCanvas, gifSize * 2, 0, gifSize, gifSize);
            } else if (gifIncludeSpectrogram) {
                const specImg = document.getElementById('specImg');
                octx.drawImage(specImg, 0, 0, gifSize, gifSize);
                octx.drawImage(ncaCanvas, gifSize, 0, gifSize, gifSize);
            } else {
                octx.drawImage(ncaCanvas, 0, 0, gifSize, gifSize);
            }
            gifEncoder.addFrame(octx, { copy: true, delay: 33 });
        };

        function stopRecording() {
            if (!gifEncoder) return;
            isRecording = false;
            recordBtn.textContent = 'Saving...';
            recordBtn.disabled = true;

            gifEncoder.on('finished', (blob) => {
                const url = URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = 'nca_recording.gif';
                a.click();
                URL.revokeObjectURL(url);
                recordBtn.textContent = 'Record GIF';
                recordBtn.style.background = '';
                recordBtn.disabled = false;
            });
            gifEncoder.render();
            gifEncoder = null;
        }

        recordBtn.addEventListener('click', async () => {
            if (isRecording) {
                stopRecording();
            } else {
                await startRecording();
            }
        });
    }

    // -----------------------------------------------------------------------
    // Simulation loop
    // -----------------------------------------------------------------------
    let frameCounter = 0;

    async function simulationLoop() {
        while (true) {
            if (isPlaying && !isLoading && cachedParams) {
                frameCounter += playbackSpeed;
                while (frameCounter >= 1) {
                    await stepNca();
                    currentFrameIdx++;
                    frameCounter -= 1;
                }
            }

            if (!isLoading) {
                renderNcaCanvas();
                if (currentMode === 'sequence') renderGtCanvas();
                document.getElementById('frameNum').textContent = currentFrameIdx;
                captureGifFrame();
            }

            await new Promise(r => setTimeout(r, 1000 / 30));
        }
    }

    // -----------------------------------------------------------------------
    // Initialization
    // -----------------------------------------------------------------------
    async function init() {
        const statusEl = document.getElementById('connStatus');
        statusEl.textContent = 'Loading manifest...';

        try {
            manifest = await (await fetch('data/manifest.json')).json();
        } catch (e) {
            statusEl.textContent = 'Failed to load manifest.json';
            statusEl.className = 'disconnected';
            return;
        }

        // Store config
        width = manifest.width;
        height = manifest.height;
        channels = manifest.channels;
        contextFramesCount = manifest.context_frames;
        numSteps = manifest.num_steps;
        latentDim = manifest.latent_dim;
        gridChannels = manifest.grid_channels;
        numSequences = manifest.num_sequences;

        if (!GITHUB_PAGES) {
            document.getElementById('totalSeq').textContent = numSequences;
            createContextCanvases();
        }

        // Load ONNX models
        statusEl.textContent = 'Loading ONNX models...';
        try {
            decodeSession = await ort.InferenceSession.create('onnx/decode_latent.onnx');
            ncaStepSession = await ort.InferenceSession.create('onnx/nca_step.onnx');
        } catch (e) {
            statusEl.textContent = 'Failed to load ONNX models: ' + e.message;
            statusEl.className = 'disconnected';
            console.error(e);
            return;
        }

        // Initialize instrument embeddings and audio
        populateInstrumentDropdown();
        loadInstrumentEmbeddings(currentInstrument);
        loadInstrument(currentInstrument);
        updateNoiseMeter();

        // Load first sequence or start with random latent
        if (GITHUB_PAGES) {
            statusEl.textContent = 'Initializing...';
            await randomLatent();
        } else {
            statusEl.textContent = 'Loading first sequence...';
            await selectSequence(0);
        }

        statusEl.textContent = 'Ready';
        statusEl.className = 'connected';

        simulationLoop();
    }

    init();
    })();

    // =========================================================================
    // Real-Time Audio Section (separate IIFE to avoid variable conflicts)
    // =========================================================================
    (function() {
    "use strict";

    // Wait for manifest to load before initializing
    let rtManifest = null;
    let rtWidth = 0, rtHeight = 0, rtChannels = 0;
    let rtLatentDim = 0, rtGridChannels = 0, rtNumSteps = 0;
    let rtEncoderInputChannels = 0;

    // ONNX sessions
    let rtEncodeSession = null;
    let rtDecodeSession = null;
    let rtNcaSession = null;

    // Audio state
    let rtAudioContext = null;
    let rtAnalyser = null;
    let rtSpectrogramBuffer = null;
    const SPEC_SIZE = 32;  // Match grid size

    // NCA state (persistent across encoder updates)
    let rtGrid = null;           // Full grid [gridChannels * H * W]
    let rtCachedParams = null;   // NCA weights from decoder
    let rtNcaFrame = null;       // RGB display buffer
    let rtHidden = null;         // Hidden channels

    // Timing
    let rtLastEncoderRun = 0;
    let rtEncoderThrottleMs = 1500;  // Updated by slider
    let rtRunning = false;

    // Canvas refs
    const rtSpecCanvas = document.getElementById('rtSpecCanvas');
    const rtNcaCanvas = document.getElementById('rtNcaCanvas');
    const rtSpecCtx = rtSpecCanvas ? rtSpecCanvas.getContext('2d') : null;
    const rtNcaCtx = rtNcaCanvas ? rtNcaCanvas.getContext('2d') : null;

    function sigmoid(x) {
        return 1 / (1 + Math.exp(-x));
    }

    function randn() {
        const u1 = Math.random(), u2 = Math.random();
        return Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);
    }

    // -----------------------------------------------------------------------
    // Audio Capture
    // -----------------------------------------------------------------------
    async function initMicrophone() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            rtAudioContext = new AudioContext();
            const source = rtAudioContext.createMediaStreamSource(stream);
            rtAnalyser = rtAudioContext.createAnalyser();
            rtAnalyser.fftSize = 2048;
            rtAnalyser.smoothingTimeConstant = 0.8;
            source.connect(rtAnalyser);
            rtSpectrogramBuffer = new Float32Array(SPEC_SIZE * SPEC_SIZE).fill(0);
            return true;
        } catch (e) {
            console.error('Microphone init failed:', e);
            return false;
        }
    }

    function computeSpectrogram() {
        if (!rtAnalyser) return null;

        const freqData = new Float32Array(rtAnalyser.frequencyBinCount);
        rtAnalyser.getFloatFrequencyData(freqData);  // dB values

        // Simple mel-scale-ish mapping to SPEC_SIZE bins
        const melBins = new Float32Array(SPEC_SIZE);
        for (let i = 0; i < SPEC_SIZE; i++) {
            // Mel-scale approximation: lower bins spread more
            const melIdx = Math.pow(i / SPEC_SIZE, 1.5) * freqData.length * 0.5;
            const bin = Math.floor(melIdx);
            // Normalize: getFloatFrequencyData returns dB, typically -100 to 0
            melBins[i] = Math.max(0, Math.min(1, (freqData[bin] + 100) / 100));
        }

        // Shift buffer left (time axis), add new column on right
        rtSpectrogramBuffer.copyWithin(0, SPEC_SIZE);
        rtSpectrogramBuffer.set(melBins, (SPEC_SIZE - 1) * SPEC_SIZE);

        // Create image with freq (y) vs time (x), low freq at bottom
        const img = new Float32Array(SPEC_SIZE * SPEC_SIZE);
        for (let t = 0; t < SPEC_SIZE; t++) {
            for (let f = 0; f < SPEC_SIZE; f++) {
                // Flip vertically so low freq at bottom
                img[(SPEC_SIZE - 1 - f) * SPEC_SIZE + t] = rtSpectrogramBuffer[t * SPEC_SIZE + f];
            }
        }
        return img;
    }

    // -----------------------------------------------------------------------
    // Rendering
    // -----------------------------------------------------------------------
    function renderSpectrogramCanvas(specImg) {
        if (!rtSpecCtx || !specImg) return;

        const displayW = rtSpecCanvas.width;
        const displayH = rtSpecCanvas.height;
        const imageData = rtSpecCtx.createImageData(displayW, displayH);
        const scaleX = displayW / SPEC_SIZE;
        const scaleY = displayH / SPEC_SIZE;

        for (let y = 0; y < displayH; y++) {
            for (let x = 0; x < displayW; x++) {
                const srcX = Math.floor(x / scaleX);
                const srcY = Math.floor(y / scaleY);
                const srcIdx = srcY * SPEC_SIZE + srcX;
                const dstIdx = (y * displayW + x) * 4;

                // Viridis-ish colormap
                const v = specImg[srcIdx];
                imageData.data[dstIdx]     = Math.floor(v * 100 + (1 - v) * 68);   // R
                imageData.data[dstIdx + 1] = Math.floor(v * 220 + (1 - v) * 1);    // G
                imageData.data[dstIdx + 2] = Math.floor(v * 150 + (1 - v) * 84);   // B
                imageData.data[dstIdx + 3] = 255;
            }
        }
        rtSpecCtx.putImageData(imageData, 0, 0);
    }

    function renderRtNcaCanvas() {
        if (!rtNcaCtx || !rtNcaFrame) return;

        const displayW = rtNcaCanvas.width;
        const displayH = rtNcaCanvas.height;
        const imageData = rtNcaCtx.createImageData(displayW, displayH);
        const scaleX = displayW / rtWidth;
        const scaleY = displayH / rtHeight;

        for (let y = 0; y < displayH; y++) {
            for (let x = 0; x < displayW; x++) {
                const srcX = Math.floor(x / scaleX);
                const srcY = Math.floor(y / scaleY);
                const dstIdx = (y * displayW + x) * 4;

                for (let c = 0; c < rtChannels; c++) {
                    const srcIdx = c * rtHeight * rtWidth + srcY * rtWidth + srcX;
                    const val = Math.min(255, Math.max(0, Math.round(rtNcaFrame[srcIdx] * 255)));
                    imageData.data[dstIdx + c] = val;
                }
                if (rtChannels === 1) {
                    imageData.data[dstIdx + 1] = imageData.data[dstIdx];
                    imageData.data[dstIdx + 2] = imageData.data[dstIdx];
                }
                imageData.data[dstIdx + 3] = 255;
            }
        }
        rtNcaCtx.putImageData(imageData, 0, 0);
    }

    // -----------------------------------------------------------------------
    // ONNX Inference
    // -----------------------------------------------------------------------
    let rtProjectionSession = null;

    async function rtLoadModels(useRtDecoder, useProjection) {
        rtEncodeSession = await ort.InferenceSession.create('onnx/encode.onnx');
        // Use separate realtime decoder if available, otherwise fall back to main decoder
        const decoderPath = useRtDecoder ? 'onnx/rt_decode_latent.onnx' : 'onnx/decode_latent.onnx';
        rtDecodeSession = await ort.InferenceSession.create(decoderPath);
        rtNcaSession = await ort.InferenceSession.create('onnx/nca_step.onnx');
        // Load projection if available
        if (useProjection) {
            rtProjectionSession = await ort.InferenceSession.create('onnx/projection.onnx');
        }
    }

    async function rtEncodeAndUpdateWeights(specImg) {
        // Build input tensor: tile single-channel spec to match encoder input channels
        const input = new Float32Array(rtEncoderInputChannels * rtHeight * rtWidth);
        for (let c = 0; c < rtEncoderInputChannels; c++) {
            for (let y = 0; y < rtHeight; y++) {
                for (let x = 0; x < rtWidth; x++) {
                    // Map SPEC_SIZE -> rtWidth/rtHeight
                    const srcY = Math.floor(y * SPEC_SIZE / rtHeight);
                    const srcX = Math.floor(x * SPEC_SIZE / rtWidth);
                    input[c * rtHeight * rtWidth + y * rtWidth + x] = specImg[srcY * SPEC_SIZE + srcX];
                }
            }
        }

        const tensor = new ort.Tensor('float32', input, [1, rtEncoderInputChannels, rtHeight, rtWidth]);
        const encResult = await rtEncodeSession.run({ spectrogram: tensor });
        let z = encResult.z.data;

        // Apply projection if available (maps encoder latent to decoder latent space)
        if (rtProjectionSession) {
            const zSrcTensor = new ort.Tensor('float32', z, [1, rtLatentDim]);
            const projResult = await rtProjectionSession.run({ z_source: zSrcTensor });
            z = projResult.z_target.data;
        }


        // Decode z -> NCA weights (but NOT grid - grid is preserved)
        // Note: if projection was used, z is now in target latent space
        const decoderLatentDim = rtProjectionSession ? z.length : rtLatentDim;
        const zTensor = new ort.Tensor('float32', z, [1, decoderLatentDim]);
        const decResult = await rtDecodeSession.run({ z: zTensor });

        // Update cached params ONLY (grid state preserved)
        rtCachedParams = {
            layer1_w: decResult.layer1_w,
            layer1_b: decResult.layer1_b,
            layer2_w: decResult.layer2_w,
            layer2_b: decResult.layer2_b,
        };

        // Gradually blend current grid toward new first_frame
        const gridSize = rtGridChannels * rtHeight * rtWidth;
        const imgSize = rtChannels * rtHeight * rtWidth;
        const hiddenSize = gridSize - imgSize;

        const firstFrame = decResult.first_frame.data;
        const blendRate = 1.0;  // How fast to blend (0 = no change, 1 = instant)

        if (!rtNcaFrame) {
            // First time - initialize directly
            rtNcaFrame = new Float32Array(imgSize);
            for (let i = 0; i < imgSize; i++) rtNcaFrame[i] = firstFrame[i];
        } else {
            // Blend toward new state
            for (let i = 0; i < imgSize; i++) {
                rtNcaFrame[i] = rtNcaFrame[i] * (1 - blendRate) + firstFrame[i] * blendRate;
            }
        }

        if (!rtHidden) {
            rtHidden = new Float32Array(hiddenSize);
            for (let i = 0; i < hiddenSize; i++) rtHidden[i] = firstFrame[imgSize + i];
        } else {
            for (let i = 0; i < hiddenSize; i++) {
                rtHidden[i] = rtHidden[i] * (1 - blendRate) + firstFrame[imgSize + i] * blendRate;
            }
        }
        rtGrid = true;
    }

    async function rtStepNca() {
        if (!rtCachedParams || !rtGrid) return;

        const gridSize = rtGridChannels * rtHeight * rtWidth;
        const imgSize = rtChannels * rtHeight * rtWidth;
        const hiddenSize = gridSize - imgSize;

        // Build grid from current state
        const grid = new Float32Array(gridSize);
        grid.set(rtNcaFrame);
        grid.set(rtHidden, imgSize);

        // Run one NCA step
        const gridTensor = new ort.Tensor('float32', grid, [1, rtGridChannels, rtHeight, rtWidth]);
        const out = await rtNcaSession.run({
            grid: gridTensor,
            layer1_w: rtCachedParams.layer1_w,
            layer1_b: rtCachedParams.layer1_b,
            layer2_w: rtCachedParams.layer2_w,
            layer2_b: rtCachedParams.layer2_b,
        });

        // Squash (matches training)
        const raw = out.new_grid.data;
        for (let i = 0; i < imgSize; i++) {
            rtNcaFrame[i] = sigmoid(raw[i]);
        }
        for (let i = 0; i < hiddenSize; i++) {
            rtHidden[i] = Math.tanh(raw[imgSize + i]);
        }
    }

    async function rtInitGrid() {
        // Initialize from random z (used on first enable)
        const z = new Float32Array(rtLatentDim);
        for (let i = 0; i < rtLatentDim; i++) z[i] = randn() * 0.5;

        const zTensor = new ort.Tensor('float32', z, [1, rtLatentDim]);
        const result = await rtDecodeSession.run({ z: zTensor });

        const gridSize = rtGridChannels * rtHeight * rtWidth;
        const imgSize = rtChannels * rtHeight * rtWidth;
        const hiddenSize = gridSize - imgSize;

        rtGrid = new Float32Array(result.first_frame.data);
        rtCachedParams = {
            layer1_w: result.layer1_w,
            layer1_b: result.layer1_b,
            layer2_w: result.layer2_w,
            layer2_b: result.layer2_b,
        };

        rtNcaFrame = new Float32Array(imgSize);
        rtHidden = new Float32Array(hiddenSize);
        for (let i = 0; i < imgSize; i++) rtNcaFrame[i] = rtGrid[i];
        for (let i = 0; i < hiddenSize; i++) rtHidden[i] = rtGrid[imgSize + i];
    }

    // -----------------------------------------------------------------------
    // Main Loop
    // -----------------------------------------------------------------------
    async function rtMicrophoneLoop() {
        if (!rtRunning || !rtAnalyser) return;

        const specImg = computeSpectrogram();
        if (specImg) {
            renderSpectrogramCanvas(specImg);

            const now = performance.now();
            if (now - rtLastEncoderRun > rtEncoderThrottleMs) {
                await rtEncodeAndUpdateWeights(specImg);
                rtLastEncoderRun = now;
            }
        }

        // Run NCA step with current (possibly just-updated) params
        if (rtCachedParams && rtNcaFrame) {
            await rtStepNca();
        }

        renderRtNcaCanvas();
        requestAnimationFrame(rtMicrophoneLoop);
    }

    // -----------------------------------------------------------------------
    // UI Event Handlers
    // -----------------------------------------------------------------------
    const rtEnableMicBtn = document.getElementById('rtEnableMicBtn');
    const rtMicStatus = document.getElementById('rtMicStatus');

    if (rtEnableMicBtn) {
        rtEnableMicBtn.addEventListener('click', async () => {
            if (rtRunning) {
                // Stop
                rtRunning = false;
                if (rtAudioContext) {
                    rtAudioContext.close();
                    rtAudioContext = null;
                    rtAnalyser = null;
                }
                rtEnableMicBtn.textContent = 'Enable Microphone';
                rtEnableMicBtn.classList.remove('active');
                rtMicStatus.textContent = '';
                return;
            }

            rtEnableMicBtn.disabled = true;
            rtMicStatus.textContent = 'Loading...';

            try {
                // Load manifest if not already loaded
                if (!rtManifest) {
                    const resp = await fetch('data/manifest.json');
                    rtManifest = await resp.json();
                    rtWidth = rtManifest.width;
                    rtHeight = rtManifest.height;
                    rtChannels = rtManifest.channels;
                    rtLatentDim = rtManifest.latent_dim;
                    rtGridChannels = rtManifest.grid_channels;
                    rtNumSteps = rtManifest.num_steps;
                    rtEncoderInputChannels = rtManifest.encoder_input_channels || (rtManifest.context_frames * rtChannels);
                }

                // Load models if not already loaded
                if (!rtEncodeSession) {
                    rtMicStatus.textContent = 'Loading models...';
                    const useRtDecoder = rtManifest.rt_decoder_available || false;
                    const useProjection = rtManifest.projection_available || false;
                    await rtLoadModels(useRtDecoder, useProjection);
                }

                // Init microphone
                rtMicStatus.textContent = 'Requesting mic...';
                const ok = await initMicrophone();
                if (!ok) {
                    rtMicStatus.textContent = 'Mic access denied';
                    rtMicStatus.classList.add('error');
                    rtEnableMicBtn.disabled = false;
                    return;
                }

                // Init grid if not already
                if (!rtGrid) {
                    rtMicStatus.textContent = 'Initializing grid...';
                    await rtInitGrid();
                }

                rtRunning = true;
                rtEnableMicBtn.textContent = 'Disable Microphone';
                rtEnableMicBtn.classList.add('active');
                rtMicStatus.textContent = 'Active';
                rtMicStatus.classList.remove('error');

                rtMicrophoneLoop();
            } catch (e) {
                console.error('RT Audio init error:', e);
                rtMicStatus.textContent = 'Error: ' + e.message;
                rtMicStatus.classList.add('error');
            }

            rtEnableMicBtn.disabled = false;
        });
    }

    // Sample rate slider
    const rtSampleRateSlider = document.getElementById('rtSampleRateSlider');
    const rtSampleRateValue = document.getElementById('rtSampleRateValue');
    if (rtSampleRateSlider) {
        rtSampleRateSlider.addEventListener('input', () => {
            rtEncoderThrottleMs = parseInt(rtSampleRateSlider.value);
            rtSampleRateValue.textContent = rtEncoderThrottleMs + 'ms';
        });
    }


    })();
    </script>
</body>
</html>