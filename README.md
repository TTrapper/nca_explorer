# Neural Cellular Automata

The demo above generates a unique Neural Cellular Automaton for each sound you play. The NCA runs in real-time, creating evolving patterns based on the audio's spectral fingerprint.

## How to Play

- **Piano keys**: Click or use keyboard (A-L for white keys, W/E/R/Y/U/O/P for black keys). Each note loads a different latent vector that generates a unique NCA.
- **Chords**: Hold multiple keys to blend their latents together, creating hybrid dynamics.
- **Instruments**: Use the dropdown to switch between piano, violin, guitar, and more. Each instrument produces distinct visual behaviors.
- **Perturbation**: Defaults to max. Press number keys 1-9 to reduce, or 0 for max. Higher values create more chaotic, unpredictable patterns. Press the same key twice to toggle off.
- **Random Latent**: Click "Random Latent" to explore completely random points in the learned space.

---

## Cellular Automata

Cellular automata are systems of simple cells on a grid, each updating its state based on neighboring values. Classic examples like Conway's Game of Life show how complex global behavior can emerge from purely local rules.

Whereas Conway's Game of Life is the application of one possible update rule, we can think of the space of all possible rules as the set of all state transition functions. For example, a binary CA where the update rule depends on the Moore neighborhood has 2^1024 possible rules. There are 9 cells in a neighborhood, giving us 2^10 (1024) entries per rule. Each entry maps the 9-cell state at time *t* to the center cell's state at time *t+1*. The rule is then convolved over the entire grid to get the new state at each time step.

![Rainbow Gliders|512x512](assets/rainbow_gliders.gif)

**Neural Cellular Automata (NCA)** replace the discrete update rules of CAs with a small neural network, allowing the system to learn its own dynamics from data. A neural network reads local values and outputs updated values for the next time step, which is fed back into the network at time *t+1*. This makes an NCA effectively a recurrent CNN! NCAs can learn to grow, regenerate, and sustain surprisingly complex dynamical patterns.

The space of all possible NCAs is infinite, even for a constrained neighborhood, because there are an infinite number of neural network architectures we could apply. For a fixed architecture, we can think of the "embedding space" of all possible parameter values. Typically, this space is pretty sparse and boring, producing mostly noise or blank outputs (although I did randomly stumble upon the "rainbow gliders" below - stable little colorful blobs that move!).


---

## Learned Latent Space

Instead of exploring the embedding space of random parameter values, we can learn a latent space and use that to *generate* NCAs. And that's what this Neuromusical Cellular Automata does:

This architecture uses a **variational autoencoder (VAE)** to map context frames into a low-dimensional latent space. A **hypernetwork** then transforms each latent vector into a unique set of NCA weights. This means every point in the latent space corresponds to a different cellular automaton with its own dynamics.

---

## Dynamics

Because NCAs are recurrent, they can learn dynamics and movement. Here is an example of a model trained on sequences of frames from dynamic simulations. Given a few context frames as input, the model learns to predict how the system evolves over time. Each training sequence captures a different behavior, forcing the model to internalize the underlying rules of motion rather than memorizing individual frames.

The ground truth is shown on the right with the model predictions on the left:

![Ground Truth Example 1|512x256](assets/groundtruth-1.gif)
![Ground Truth Example 2|512x256](assets/groundtruth-2.gif)
![Ground Truth Example 3|512x256](assets/groundtruth-3.gif)

As you can see, the NCA does capture the general size, color, and motion of the objects, although it tends to blur over time as it struggles to guess the exact next frame. Remember this is a *tiny* little network because it needs to fit on my old laptop's GTX 960 and train in reasonable time. And anyway, I kind of like how the objects smear out over time and generate interesting patterns - it makes the latent space more surprising.

---

## Music

The training data for the Neuromusical Cellular Automata and the demo at the top of this page is generated by pairing audio with deterministic visualizations.

![Ground Truth Example 1|512x256](assets/spectrogram_piano.gif)

For each instrument sample (piano, violin, etc.), we extract audio features: detected harmonics, loudness (RMS), and spectral brightness. These features drive the "circles" visualization where each harmonic becomes a colored circle - low frequencies appear warm (red/orange) near the center, high frequencies appear cool (blue) toward the edges. Circle size pulses with loudness, and positions orbit based on harmonic relationships.

![Ground Truth Example 2|512x256](assets/spectrogram_xylophone.gif)

The first frame of each training sequence is a **mel spectrogram** (a 2D image of frequency vs. time), which the encoder compresses into a 64-dimensional latent vector. The decoder, a hypernetwork, transforms this latent into NCA weights that generate the subsequent circle animation frames.

![Ground Truth Example 3|512x256](assets/spectrogram_clarinet.gif)

For the piano interface, we pre-generate spectrograms for all 19 notes (F4 through B5) across every instrument. Each spectrogram is encoded into its latent vector and stored in a manifest. When you press a piano key, the corresponding latent is loaded; pressing multiple keys (a chord) averages their latents together. This blended latent then drives the NCA in real-time, creating visualizations that interpolate between the learned behaviors of each note.

---

## Future Potential

This approach opens several interesting directions:

- The latent space could be conditioned on higher-level descriptions, allowing **natural language control** over the generated dynamics
- Larger grids and deeper NCA architectures could capture more complex phenomena
- The ability for NCAs to track/produce **agentic behavior** on the grid is particularly fascinating

I did early experiments with boids exhibiting various behaviors such as flocking or predator-prey dynamics, but this proved too challenging for my tiny 2-layer conv nets. It would be fascinating to see if an NCA could learn theory of mind to generate the actions of human players in Atari games, or even more complex environments and life-like behaviors.

Broadly, music offers a compelling interface for exploring high-dimensional latent spaces. The simple circle visualizations here map audio features to color and motion, but richer correspondences are possible. Imagine learning a shared embedding where the space of complex music aligns with the space of natural images or video - not through superficial features, but through deeper semantic structure: tension and resolution in a symphony mapping to dramatic arcs in film, the texture of a jazz improvisation corresponding to the organic chaos of a forest canopy, or the emotional trajectory of a song finding its visual analogue in shifting landscapes. Contrastive learning on large audio-visual datasets could discover these cross-modal correspondences, letting music become a navigation tool for exploring generative models of images, video, or even text - playing a melody to traverse a space of scenes that feel emotionally consonant with the sound.

The combination of neural cellular automata with learned latent spaces suggests a fascinating paradigm: compact, local update rules that are themselves generated by a learned model, producing an open-ended family of emergent systems from a single trained network.
